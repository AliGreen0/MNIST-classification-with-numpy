{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An implementation of NN using numpy for MNIST classification\n",
    "\n",
    "Note: Using ReLU activation function for the first layer might cause a runtime warning when computing the sigmoid function for further layers. This happens when normal initialization is used. It is better to use sigmoid in the first layer.\n",
    "\n",
    "Update: The previous issue is solved by using He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 09:11:12.075607: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-17 09:11:12.075638: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import time\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "print(x_train.shape) #(60000, 28, 28)\n",
    "print(x_val.shape) #(10000, 28, 28)\n",
    "\n",
    "#Flatten the inputs\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "\n",
    "\n",
    "x_train = np.reshape(x_train,(-1,28*28))\n",
    "x_val = np.reshape(x_val,(-1,28*28))\n",
    "\n",
    "#Normalize inputs\n",
    "x_train = x_train/255.0\n",
    "x_val = x_val/255.0\n",
    "\n",
    "print(x_train.shape) #(60000, 784)\n",
    "print(x_val.shape) #(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, epochs=10, l_rate=0.045):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "\n",
    "        # we save all parameters in the neural network in this dictionary\n",
    "        self.params = self.initialization()\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def relu(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            #temp = np.zeros((x.shape))\n",
    "            temp = (x > 1)*1\n",
    "            return temp\n",
    "        return np.maximum(x,0)\n",
    "    \n",
    "    \n",
    "    def initialization(self):\n",
    "        # He initialization is used\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        hidden_2=self.sizes[2]\n",
    "        output_layer=self.sizes[3]\n",
    "\n",
    "        params = {\n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(2/self.sizes[0]),\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(2/self.sizes[1]),\n",
    "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(2/self.sizes[2]),\n",
    "            \n",
    "            'b1':np.zeros((hidden_1,1)),\n",
    "            'b2':np.zeros((hidden_2,1)),\n",
    "            'b3':np.zeros((output_layer,1))\n",
    "        }\n",
    "        \n",
    "\n",
    "        return params\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "\n",
    "        # input layer activations becomes sample\n",
    "        params['A0'] = x_train[:,None]\n",
    "        # (784,1)\n",
    "        #print(params['A0'].shape)\n",
    "\n",
    "        # input layer to hidden layer 1\n",
    "        params['Z1'] = np.dot(params[\"W1\"], params['A0']) + params['b1']\n",
    "        ##()\n",
    "        params['A1'] = self.relu(params['Z1'])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        params['Z2'] = np.dot(params[\"W2\"], params['A1']) + params['b2']\n",
    "        params['A2'] = self.relu(params['Z2'])\n",
    "\n",
    "        # hidden layer 2 to output layer\n",
    "        params['Z3'] = np.dot(params[\"W3\"], params['A2']) + params['b3']\n",
    "        params['A3'] = self.sigmoid(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "\n",
    "    def backward_pass(self, y_train, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "        \n",
    "        #y_train_back = y_train[:,None]\n",
    "        y_train_back = y_train[:,None]\n",
    "\n",
    "        d_Z3 = output - y_train_back\n",
    " \n",
    "        d_W3 = np.dot(d_Z3, params['A2'].T)\n",
    "        d_b3 = d_Z3\n",
    "                \n",
    "        change_w['W3'] = d_W3\n",
    "        change_w['b3'] = d_b3\n",
    "        #change_w['b3'] = d_b3\n",
    "\n",
    "        # Calculate W2 update\n",
    "        d_Z2 = np.dot(params['W3'].T, d_Z3) * self.relu(params['Z2'], derivative=True)\n",
    "        d_W2 = np.dot(d_Z2, params['A1'].T)\n",
    "        d_b2 = d_Z2\n",
    "        \n",
    "        change_w['W2'] = d_W2\n",
    "        change_w['b2'] = d_b2\n",
    "        \n",
    "        # Calculate W1 update\n",
    "        d_Z1 = np.dot(params['W2'].T, d_Z2) * self.relu(params['Z1'], derivative=True)\n",
    "        \n",
    "        d_W1 = np.dot(d_Z1, params['A0'].T)\n",
    "        d_b1 = d_Z1\n",
    "        \n",
    "        change_w['b1'] = d_b1\n",
    "        change_w['W1'] = d_W1\n",
    "\n",
    "        return change_w\n",
    "\n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        '''\n",
    "            Update network parameters according to update rule from\n",
    "            Stochastic Gradient Descent.\n",
    "\n",
    "            θ = θ - η * ∇J(x, y), \n",
    "                theta θ:            a network parameter (e.g. a weight w)\n",
    "                eta η:              the learning rate\n",
    "                gradient ∇J(x, y):  the gradient of the objective function,\n",
    "                                    i.e. the change for a specific theta θ\n",
    "        '''\n",
    "        \n",
    "        for key, value in changes_to_w.items():\n",
    "            self.params[key] -= self.l_rate * value\n",
    "\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        for iteration in range(self.epochs):\n",
    "            for x,y in zip(x_train, y_train):\n",
    "                output = self.forward_pass(x)\n",
    "                changes_to_w = self.backward_pass(y, output)\n",
    "                self.update_network_parameters(changes_to_w)\n",
    "            \n",
    "            predictions = []\n",
    "            for x,y in zip(x_val,y_val):\n",
    "                output = self.forward_pass(x)\n",
    "                pred = np.argmax(output)\n",
    "                predictions.append(pred == np.argmax(y))\n",
    "            \n",
    "            accuracy = np.mean(predictions)\n",
    "            print('Epoch: {0},  Accuracy: {1:.2f}%'.format(\n",
    "                iteration+1, accuracy * 100\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1,  Accuracy: 90.06%\n",
      "Epoch: 2,  Accuracy: 92.86%\n",
      "Epoch: 3,  Accuracy: 93.76%\n",
      "Epoch: 4,  Accuracy: 94.79%\n",
      "Epoch: 5,  Accuracy: 95.28%\n",
      "Epoch: 6,  Accuracy: 95.58%\n",
      "Epoch: 7,  Accuracy: 95.85%\n",
      "Epoch: 8,  Accuracy: 96.22%\n",
      "Epoch: 9,  Accuracy: 96.28%\n",
      "Epoch: 10,  Accuracy: 96.47%\n"
     ]
    }
   ],
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10],epochs=10,l_rate=0.001)\n",
    "dnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
